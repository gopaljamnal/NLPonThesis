{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "id": "edKgz1VfcYwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GEmIG7NFboqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "5Mezj2oJcd7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/PhD_thesis.pdf'\n",
        "open_filename = open(filename, 'rb')\n",
        "\n",
        "ind_manifesto = PyPDF2.PdfReader(open_filename)"
      ],
      "metadata": {
        "id": "SyTiaCbdc165"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ind_manifesto.metadata"
      ],
      "metadata": {
        "id": "1pho_eiDeQWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_pages = len(ind_manifesto.pages)\n",
        "total_pages"
      ],
      "metadata": {
        "id": "_ak2JUSveS9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textract"
      ],
      "metadata": {
        "id": "cez-ngGQeW2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textract"
      ],
      "metadata": {
        "id": "7PhOO_Rjeolz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "text  = ''\n",
        "\n",
        "# Lets loop through, to read each page from the pdf file\n",
        "while(count < total_pages):\n",
        "    # Get the specified number of pages in the document\n",
        "    mani_page  = ind_manifesto.pages[count]\n",
        "    # Process the next page\n",
        "    count += 1\n",
        "    # Extract the text from the page\n",
        "    text += mani_page.extract_text()"
      ],
      "metadata": {
        "id": "_e6TE_8deq8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if text != '':\n",
        "    text = text\n",
        "    \n",
        "else:\n",
        "    textract.process(open_filename, method='tesseract', encoding='utf-8', langauge='eng' )    "
      ],
      "metadata": {
        "id": "cw91uH6PfYLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autocorrect"
      ],
      "metadata": {
        "id": "nTJecn0qfdbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "AbuAj19eful4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autocorrect import Speller\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "def to_lower(text):\n",
        "\n",
        "    \"\"\"\n",
        "    Converting text to lower case as in, converting \"Hello\" to  \"hello\" or \"HELLO\" to \"hello\".\n",
        "    \"\"\"\n",
        "    \n",
        "    # Specll check the words\n",
        "    spell  = Speller(lang='en')\n",
        "    \n",
        "    texts = spell(text)\n",
        "    \n",
        "    return ' '.join([w.lower() for w in word_tokenize(text)])\n",
        "\n",
        "lower_case = to_lower(text)\n",
        "print(lower_case)"
      ],
      "metadata": {
        "id": "EgLtE7WXfg7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords, brown\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from autocorrect import spell\n",
        "\n",
        "lower_case"
      ],
      "metadata": {
        "id": "TTblDzkOgHUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(lower_case):\n",
        "    # split text phrases into words\n",
        "    words  = nltk.word_tokenize(lower_case)\n",
        "    \n",
        "    \n",
        "    # Create a list of all the punctuations we wish to remove\n",
        "    punctuations = ['.', ',', '/', '!', '?', ';', ':', '(',')', '[',']', '-', '_', '%']\n",
        "    \n",
        "    # Remove all the special characters\n",
        "    punctuations = re.sub(r'\\W', ' ', str(lower_case))\n",
        "    \n",
        "    # Initialize the stopwords variable, which is a list of words ('and', 'the', 'i', 'yourself', 'is') that do not hold much values as key words\n",
        "    stop_words  = stopwords.words('english')\n",
        "    \n",
        "    # Getting rid of all the words that contain numbers in them\n",
        "    w_num = re.sub('\\w*\\d\\w*', '', lower_case).strip()\n",
        "    \n",
        "    # remove all single characters\n",
        "    lower_case = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', lower_case)\n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    lower_case = re.sub(r'\\s+', ' ', lower_case, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    lower_case = re.sub(r'^b\\s+', '', lower_case)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Removing non-english characters\n",
        "    lower_case = re.sub(r'^b\\s+', '', lower_case)\n",
        "    \n",
        "    # Return keywords which are not in stop words \n",
        "    keywords = [word for word in words if not word in stop_words  and word in punctuations and  word in w_num]\n",
        "    \n",
        "    return keywords"
      ],
      "metadata": {
        "id": "QPZWFODUgLx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize the words\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in clean_text(lower_case)]\n",
        "\n",
        "# lets print out the output from our function above and see how the data looks like\n",
        "clean_data = ' '.join(lemmatized_word)\n",
        "print(clean_data)"
      ],
      "metadata": {
        "id": "U32SSZ4mgPnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize the words\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_word = [wordnet_lemmatizer.lemmatize(word) for word in clean_text(lower_case)]\n",
        "\n",
        "# lets print out the output from our function above and see how the data looks like\n",
        "clean_data = ' '.join(lemmatized_word)\n",
        "print(clean_data)"
      ],
      "metadata": {
        "id": "AKh703Q7gXQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "59v58HuJgwzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame([clean_data])\n",
        "df.columns = ['script']\n",
        "df.index = ['Itula']\n",
        "df"
      ],
      "metadata": {
        "id": "5i5rM6txg4vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Counting the occurrences of tokens and building a sparse matrix of documents x tokens.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "corpus = df.script\n",
        "vect = CountVectorizer(stop_words='english')\n",
        "\n",
        "# Transforms the data into a bag of words\n",
        "data_vect = vect.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "Rpwt4Hong-U4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vect.get_feature_names_out()\n",
        "data_vect_feat = pd.DataFrame(data_vect.toarray(), columns=feature_names)\n",
        "data_vect_feat.index = df.index\n",
        "data_vect_feat"
      ],
      "metadata": {
        "id": "RZR6JvxchBNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_vect_feat.transpose()\n",
        "data"
      ],
      "metadata": {
        "id": "Xg7PLdKxhEgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "# Find the top 1000 words written in the manifesto\n",
        "top_dict = {}\n",
        "for c in data.columns:\n",
        "    top = data[c].sort_values(ascending=False)\n",
        "    top_dict[c]= list(zip(top.index, top.values))\n",
        "\n",
        "    \n",
        "for x in list(top_dict)[0:100]:\n",
        "    print(\"key {}, value {} \".format(x,  top_dict[x]))"
      ],
      "metadata": {
        "id": "WnF3UmSXhHea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the most common top words --> add them to the stop word list\n",
        "from collections import Counter\n",
        "\n",
        "# Let's first pull out the top 100 words for each comedian\n",
        "words = []\n",
        "for president in data:\n",
        "    top = [word for (word, count) in top_dict[president]]\n",
        "    for t in top:\n",
        "        words.append(t)\n",
        "\n",
        "print(words[:10])"
      ],
      "metadata": {
        "id": "GAIhMoYXhMXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "\n",
        "\n",
        "# Get 100 words based on the \n",
        "words_except_stop_dist = nltk.FreqDist(w for w in words[:]) \n",
        "wordcloud = WordCloud(stopwords=set(STOPWORDS),background_color='black').generate(\" \".join(words_except_stop_dist))\n",
        "plt.imshow(wordcloud,interpolation = 'bilinear')\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(10,12)\n",
        "plt.axis('off')\n",
        "plt.title(\"Top most common 100 words from Dr. Itula's Manifesto 2019\",fontsize=20)\n",
        "plt.tight_layout(pad=0)\n",
        "plt.savefig('Manifesto_top_100.jpeg')"
      ],
      "metadata": {
        "id": "Og4n5zDohNp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vaderSentiment"
      ],
      "metadata": {
        "id": "tDuuC15hje3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "mEjiR-EBjh0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(clean_data)\n",
        "blob.sentiment"
      ],
      "metadata": {
        "id": "ruL-65HHjlHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing all the necessary libraries\n",
        "from nltk.cluster.util import cosine_distance\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "RtrojpUdjonM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " \n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        " \n",
        "    all_words = list(set(sent1 + sent2))\n",
        " \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        " \n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        " \n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        " \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        " \n",
        "# One out of 5 words differ => 0.8 similarity\n",
        "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split()))\n",
        " \n",
        "# One out of 2 non-stop words differ => 0.5 similarity\n",
        "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split(), stopwords.words('english')))\n",
        " \n",
        "# 0 out of 2 non-stop words differ => 1 similarity (identical sentences)\n",
        "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a good sentence\".split(), stopwords.words('english')))\n",
        " \n",
        "# Completely different sentences=> 0.0\n",
        "print(sentence_similarity(\"This is a good sentence\".split(), \"I want to go to the market\".split(), stopwords.words('english')))"
      ],
      "metadata": {
        "id": "CsOrCr-ojsg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(sentences)\n",
        "\n",
        "# get the english list of stopwords\n",
        "#stop_words = stopwords.words('english')\n",
        " \n",
        "def build_similarity_matrix(lower_case, stopwords=None):\n",
        "    # Create an empty similarity matrix\n",
        "    S = np.zeros([len(lower_case), len(lower_case)])\n",
        " \n",
        " \n",
        "    for idx1 in range(len(lower_case)):\n",
        "        for idx2 in range(len(lower_case)):\n",
        "            if idx1 == idx2:\n",
        "                continue\n",
        " \n",
        "            S[idx1][idx2] = sentence_similarity(lower_case[idx1], lower_case[idx2], stop_words)\n",
        " \n",
        "    # normalize the matrix row-wise\n",
        "    for idx in range(len(S)):\n",
        "        S[idx] /= S[idx].sum()\n",
        " \n",
        "    return S"
      ],
      "metadata": {
        "id": "T8pMC1lmj_8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(lower_case, top_n=5):\n",
        "    # Remove all the stopwords in the document\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "    \n",
        "    \n",
        "    \n",
        "    #Read text and tokenize\n",
        "    #lower_case  = nltk.word_tokenize(lower_case)\n",
        "    \n",
        "   \n",
        "    \n",
        "    #Generate similarity matrix across sentences\n",
        "    sentence_similarity  = build_similarity_matrix((lower_case, stop_words))\n",
        "    \n",
        "    #Rank sentences in similarity matrix\n",
        "    sentence_similiraty_graph = nx.from_numpy_array(sentence_similarity)\n",
        "    scores = nx.pagerank(sentence_similiraty_graph)\n",
        "    \n",
        "    \n",
        "    #Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(lower_case)), reverse=True)    \n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence) \n",
        "    \n",
        "    for i in range(top_n):\n",
        "        summarize_text.append(' '.join(ranked_sentence[i][1]))\n",
        "        \n",
        "    #Output the summarized text\n",
        "    print('Summarized Text: \\n', '. '.join(summarize_text))"
      ],
      "metadata": {
        "id": "e-Dcik9dkFw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "from gensim.summarization.summarizer import summarize"
      ],
      "metadata": {
        "id": "KXG2V74lkI5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out our summarized text of the document which was converted to lower case, remember we could have opted to remove stopwords as well.\n",
        "\n",
        "print(summarize(lower_case))"
      ],
      "metadata": {
        "id": "hv9kBGdNkKlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyLDAvis"
      ],
      "metadata": {
        "id": "KM6bW0Z3naPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gensim\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Plotting tools\n",
        "import pyLDAvis\n",
        "#import graphlab as gl\n",
        "#import pyLDAvis.graphlab\n",
        "import pyLDAvis.gensim_models  # don't skip this\n",
        "\n",
        "# Enable logging for gensim - optional\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
      ],
      "metadata": {
        "id": "bDL9jQI5kbJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data  = []\n",
        "data.append(clean_text(lower_case))"
      ],
      "metadata": {
        "id": "JdBs5AK7lS4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This time we use spacy for lemmatizarion \n",
        "import spacy\n",
        "\n",
        "# Second lemmatization of our data\n",
        "def lemmatization(data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_output = []\n",
        "    for sent in data:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
        "    return texts_output\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "# Lemmatize keeping only noun, adj, vb, adv\n",
        "data_lemmatized = lemmatization(data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
      ],
      "metadata": {
        "id": "69Bg799VlVwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Dictionary\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "\n",
        "# Create Corpus\n",
        "texts = data_lemmatized\n",
        "\n",
        "# Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in texts]\n",
        "\n",
        "\n",
        "# View\n",
        "print(corpus[:1])"
      ],
      "metadata": {
        "id": "67we8xv3mJJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA model\n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, alpha='auto', num_topics=20, random_state=100,\n",
        "                                           update_every=1, passes=20, per_word_topics=True)"
      ],
      "metadata": {
        "id": "pngBgsfBmXuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets view the topics in our model\n",
        "print(lda_model.print_topics())\n",
        "doc_lda  = lda_model[corpus]"
      ],
      "metadata": {
        "id": "ZfYVv5bymZLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print model perplexity\n",
        "print('\\nPerplexity:', lda_model.log_perplexity(corpus))\n",
        "\n",
        "\n",
        "# Coherence Score\n",
        "\n",
        "coherence_model_lda = CoherenceModel(lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score:', coherence_lda)"
      ],
      "metadata": {
        "id": "agpJ0R7AmuNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "vis_topics = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)"
      ],
      "metadata": {
        "id": "a6vOdbRUmx8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vis_topics"
      ],
      "metadata": {
        "id": "p_l6UqmZnOF4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}